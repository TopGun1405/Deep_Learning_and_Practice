{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9003a9365287adde",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### titanic_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e596cf5ab066673b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:08.905636200Z",
     "start_time": "2023-10-12T02:32:59.960079700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602431ee266789ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:11.881481100Z",
     "start_time": "2023-10-12T02:33:11.857949900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return {'input': feature, 'target': target}\n",
    "    \n",
    "    def __str__(self):\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.X), self.X.shape, self.y.shape\n",
    "        )\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6457c2ddfb81604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:13.189353100Z",
     "start_time": "2023-10-12T02:33:13.160691400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "    \n",
    "    def __str__(self):\n",
    "        str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17bda1d9314a2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:14.465408Z",
     "start_time": "2023-10-12T02:33:14.435500600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    \n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    \n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    \n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    \n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    \n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    \n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    \n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "    \n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    \n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    \n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    # print(dataset)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "    # print(test_dataset)\n",
    "    \n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d5a1d0ad53f7b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:15.582299100Z",
     "start_time": "2023-10-12T02:33:15.542823800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass별 Fare 평균값을 사용하여 Fare 결측치 메우기\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a6377e32fa617a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:16.699236100Z",
     "start_time": "2023-10-12T02:33:16.667441800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f93363b11f9dbd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:17.786240300Z",
     "start_time": "2023-10-12T02:33:17.762639100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c0c86cd0fe97c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:18.821748400Z",
     "start_time": "2023-10-12T02:33:18.789977400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    \n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "    \n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca05519399d7e5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:20.098637500Z",
     "start_time": "2023-10-12T02:33:20.071545400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "        ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "        ), \n",
    "        \"honorific\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e08ea42f214eb9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:21.172181200Z",
     "start_time": "2023-10-12T02:33:21.156156200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "            le = le.fit(all_df[category_feature])\n",
    "            all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6084851273229a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:22.158469200Z",
     "start_time": "2023-10-12T02:33:22.127220800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(30, 30), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(30, n_output), \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a1505cd6952555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:23.381414900Z",
     "start_time": "2023-10-12T02:33:23.352166700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(test_data_loader):\n",
    "    print(\"[TEST]\")\n",
    "    batch = next(iter(test_data_loader))\n",
    "    print(\"{0}\".format(batch['input'].shape))\n",
    "    my_model = MyModel(n_input=11, n_output=2)\n",
    "    output_batch = my_model(batch['input'])\n",
    "    prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "    for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "        print(idx, prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb6b146da20b27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T02:33:27.232463100Z",
     "start_time": "2023-10-12T02:33:25.083275200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     train_dataset, validation_dataset, test_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mget_preprocessed_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_dataset: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, validation_dataset.shape: \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m, test_dataset: \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28mlen\u001B[39m(train_dataset), \u001B[38;5;28mlen\u001B[39m(validation_dataset), \u001B[38;5;28mlen\u001B[39m(test_dataset)\n\u001B[0;32m      6\u001B[0m     ))\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m, in \u001B[0;36mget_preprocessed_dataset\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_preprocessed_dataset\u001B[39m():\n\u001B[1;32m----> 2\u001B[0m     CURRENT_FILE_PATH \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;18;43m__file__\u001B[39;49m))\n\u001B[0;32m      4\u001B[0m     train_data_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(CURRENT_FILE_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m     test_data_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(CURRENT_FILE_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "    \n",
    "    print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "        len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "    ))\n",
    "    print(\"#\" * 50, 1)\n",
    "    \n",
    "    for idx, sample in enumerate(train_dataset):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, sample['input'], sample['target']))\n",
    "    print(\"#\" * 50, 2)\n",
    "    \n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "    \n",
    "    print(\"[TRAIN]\")\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "    \n",
    "    print(\"[VALIDATION]\")\n",
    "    for idx, batch in enumerate(validation_data_loader):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "    \n",
    "    print(\"#\" * 50, 3)\n",
    "    \n",
    "    test(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de1a94f3fe5a8f85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### submission.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f337158cf78b51eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![kaggle rank](image/img.png)\n",
    "![kaggle rank2](image/img_1.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9a5c956bd59cac4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wandb Page"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2d99a34e0c20320"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![wandb](image/img_2.png)\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[wandb(https://wandb.ai/guns/homework2?workspace=user-fstopgun0317)](https://wandb.ai/guns/homework2?workspace=user-fstopgun0317)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc939019012d4df9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1e5bf4e3bee2ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 숙제 후기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e47c9685ffe5d74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "아직 미흡한 부분이 많아 만족스럽지는 못했지만 새롭게 모델을 구성해보게 된 것에 흥미를 느낄 수 있었다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aee917472ea3abeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
